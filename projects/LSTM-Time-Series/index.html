<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>LSTM Time Series Knowledge | Linh P. Nguyen</title> <meta name="author" content="Linh P. Nguyen"> <meta name="description" content="briefly talk about LSTM time series with example"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9C%BF&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://npl0204.github.io/projects/LSTM-Time-Series/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Linh </span>P. Nguyen</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">LSTM Time Series Knowledge</h1> <p class="post-description">briefly talk about LSTM time series with example</p> </header> <article> <h2 id="lstm"><strong>LSTM</strong></h2> <h4 id="lstm-is-a-variant-of-rnn-used-in-deep-learning-and-can-be-used-lstms-when-working-on-sequences-of-data"> <strong>LSTM</strong> is a variant of RNN used in deep learning, and can be used LSTMs when working on sequences of data.</h4> <ul> <li>Time series forecasting (for example, stock prediction)</li> <li>Text generation</li> <li>Video classification</li> <li>Music Generation</li> <li>Anomaly detection</li> </ul> <h4 id="rnns-are-neural-networks-that-are-good-with-sequential-data-it-can-be-video-audio-text-stock-market-time-series-or-even-a-single-image-cut-into-a-sequence-of-its-parts"> <strong>RNNs</strong> are neural networks that are good with sequential data. It can be video, audio, text, stock market time series, or even a single image cut into a sequence of its parts.</h4> <p><em>Compare:</em> Standard neural networks (convolutional or vanilla) have one major shortcoming when compared to RNNs - they cannot reason about previous inputs to inform later ones.</p> <h4 id="lstm-vs-rnn"><strong>LSTM vs RNN</strong></h4> <ul> <li>Typical RNNs can’t memorize long sequences. The effect called “vanishing gradients” happens during the backpropagation phase of the RNN cell network. The gradients of cells that carry information from the start of a sequence go through matrix multiplications by small numbers and reach close to 0 in long sequences. In other words - information at the start of the sequence has almost no effect at the end of the sequence.</li> <li>LSTM is an RNN architecture that can memorize long sequences - up to 100s of elements in a sequence. LSTM has a memory gating mechanism that allows the long-term memory to continue flowing into the LSTM cells.</li> </ul> <h4 id="core-idea"><strong>Core idea</strong></h4> <ul> <li>The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.</li> </ul> <h4 id="memory-gate"><strong>Memory Gate</strong></h4> <ul> <li>Information in LSTMs can be stored, written, or read via gates that open and close. These gates store the memory in the <strong>analog</strong> format, implementing element-wise multiplication by sigmoid ranges between 0-1. Analog, being differentiable in nature, is suitable for backpropagation.</li> <li>Architecture of LSTM: <blockquote> <ol> <li>Tanh: Non-linear activation function. It regulates the values flowing through the network, maintaining the values between <strong>-1 and 1</strong>.&lt;/li&gt;</li> <li>Sigmoid: Non-linear activation functions. It is contained by the gate, values between <strong>0 and 1</strong>. It helps the network to <strong>update</strong> or <strong>forget</strong> the data. <strong>multiplication</strong> = <strong>0</strong> -&gt; <strong>forgotten</strong>; = <strong>1</strong> -&gt; <strong>information stays</strong>.</li> </ol> </blockquote> </li> <li> <strong>Forget gate</strong>: input: current and hidden; output: f(t) values between 0 and 1 <blockquote> <ul> <li> <strong>Decide</strong> which <strong>information</strong> needs attention and which can be ignored.</li> <li>The information from the current input X(t) and hidden state h(t-1) are passed through the sigmoid function. <blockquote> <p>Sigmoid generates values between <strong>0 and 1</strong>. It concludes whether the part of the old output is necessary (by giving the output closer to 1). This value of f(t) will later be used by the cell for point-by-point multiplication.</p> </blockquote> </li> </ul> </blockquote> </li> <li> <strong>Input gate</strong>: input: current state X(t) and previously hidden state h(t-1); output: vector (C~(t)) with all the possible values between -1 and 1 <blockquote> <ul> <li> <strong>Update</strong> the <strong>cell status</strong>.</li> <li>First, the current state X(t) and previously hidden state h(t-1) are passed into the second sigmoid function. The values are transformed between 0 (important) and 1 (not important). Next, the same information of the hidden state and current state will be passed through the tanh function which will create a vector (C~(t)) with all the possible values between <strong>-1 and 1</strong>. The output values generated from the activation functions are ready for point-by-point multiplication.</li> </ul> </blockquote> </li> <li> <strong>Cell state operation</strong>: input: new state after forget and input; output: updates the cell state to C(t) <blockquote> <ul> <li> <strong>Decide and store the information</strong> from the new state in the cell state, after having information from the forget and input gate.</li> <li>The previous cell state C(t-1) gets multiplied with forget vector f(t). If the outcome is 0, then values will get dropped in the cell state.</li> <li>Next, the network takes the output value of the input vector i(t) and performs point-by-point addition, which updates the cell state giving the network a new cell state C(t).</li> </ul> </blockquote> </li> <li> <strong>Output gate</strong>: input: values of the current state and previous hidden state; output: new cell state and new hidden state <blockquote> <ul> <li> <strong>Determines</strong> the value of the <strong>next hidden state</strong>. This state <strong>contains information on previous inputs</strong>.</li> <li>First, the values of the current state and previous hidden state are passed into the third sigmoid function. Then the new cell state generated from the cell state is passed through the tanh function. Both these outputs are multiplied point-by-point. Based on the final value, the network decides which information the hidden state should carry. This <strong>hidden state is used for prediction</strong>.</li> <li>Finally, the <strong>new cell state and new hidden state</strong> are carried over to the next time step.</li> </ul> </blockquote> </li> <li>Overall, <strong>forget gate</strong> <strong>determines</strong> which <strong>relevant information</strong> from the prior steps is needed. The <strong>input gate</strong> <strong>decides</strong> what <strong>relevant information can be added</strong> from the current step, and the <strong>output gate</strong> <strong>finalizes</strong> the <strong>next hidden state</strong>.</li> </ul> <div class="col"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/21-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/21-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/21-1400.webp"></source> <img src="/assets/img/21.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="hyperparameters-to-tune"><strong><a href="https://medium.com/geekculture/10-hyperparameters-to-keep-an-eye-on-for-your-lstm-model-and-other-tips-f0ff5b63fcd4" rel="external nofollow noopener" target="_blank">Hyperparameters to tune</a></strong></h4> <ul> <li>Number of nodes and hidden layers.</li> <li>Number of units in dease layer: A dense layer is the most frequently used layer which is basically a layer where each neuron receives input from all neurons in the previous layer.</li> <li>Dropout: Helps avoid overfitting in training by bypassing randomly selected neurons, thereby reducing the sensitivity to specific weights of the individual neurons.</li> <li>Activation functions.</li> <li>Learning rate: Setting a higher learning rate accelerates the learning but the model may not converge. Conversely, a lower rate will slow down the learning drastically as steps towards the minimum of loss function will be tiny but will allow the model to converge smoothly.</li> <li>Batch size: Large sizes make large gradient steps compared to smaller ones for the same number of samples “seen”.</li> </ul> <h2 id="lstm-rnn-beer-wine-and-distilled-alcoholic-beverages-sales"><strong>LSTM RNN Beer Wine and Distilled Alcoholic Beverages Sales</strong></h2> <p><a href="https://colab.research.google.com/drive/1LuKKgHzgzxYDxnGdaNpC3EXgxU1XzS47?usp=sharing" rel="external nofollow noopener" target="_blank">An example project</a> (please use Google Colab Notebook) employed LSTM time series RNN to forecast wine prices, leveraging the temporal patterns in historical wine pricing data</p> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"alshedivat/al-folio","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Linh P. Nguyen. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: December 06, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>